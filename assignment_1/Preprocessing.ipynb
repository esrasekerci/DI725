{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242d4643",
   "metadata": {},
   "source": [
    "## Strategy Summary\n",
    "\n",
    "This notebook follows an optimal strategy for preprocessing customer-agent dialogues for sentiment classification:\n",
    "\n",
    "- Retains `customer:` and `agent:` tags to preserve speaker roles.\n",
    "- Removes generic agent greetings to avoid introducing bias.\n",
    "- Strips noise: punctuation, emails, phone numbers, URLs, and excess whitespace.\n",
    "- Removes known polite/boilerplate phrases that add no sentiment signal.\n",
    "- Fixes common spelling issues that harm tokenization.\n",
    "- Splits the dataset using stratified sampling to balance sentiment classes.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6749a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18027024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and encode labels\n",
    "df = pd.read_csv(\"data/train.csv\")[[\"customer_sentiment\", \"conversation\"]].dropna()\n",
    "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['label'] = df['customer_sentiment'].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dcab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_phrases = [\n",
    "    r'\\bis there anything else i can (assist|help) you with\\b',\n",
    "    r'\\bthank you for choosing brownbox\\b',\n",
    "    r'\\bthank you\\b',\n",
    "    r'\\byoure welcome\\b',\n",
    "    r'\\btake care\\b',\n",
    "    r'\\bgoodbye\\b',\n",
    "    r'\\bplease\\b',\n",
    "    r'\\bthanks\\b',\n",
    "    r'\\bsure\\b',\n",
    "    r'\\bno thats all\\b',\n",
    "    r'\\bhave a (nice|great|good) day\\b',\n",
    "    r'\\bappreciate\\b',\n",
    "    r'\\bfor contacting brownbox customer support\\b'\n",
    "]\n",
    "\n",
    "misspellings = {\n",
    "    'ts': 'this', 'witn': 'within', 'anytng': 'anything',\n",
    "    'ithis': 'it has', 'thathis': 'that is', 'as you': 'assure you',\n",
    "    'en that': 'ensure that'\n",
    "}\n",
    "\n",
    "def clean_conversation(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\+?\\d[\\d\\s\\-().]{8,}\\d', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    if lines and lines[0].startswith(\"agent:\"):\n",
    "        if any(greet in lines[0] for greet in [\n",
    "            \"thank you for calling\", \"hi\", \"hello\", \"this is\", \"my name is\",\n",
    "            \"how can i help you\", \"how may i assist you\"\n",
    "        ]):\n",
    "            lines = lines[1:]\n",
    "\n",
    "    text = ' '.join(lines)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    for phrase in custom_phrases:\n",
    "        text = re.sub(phrase, '', text)\n",
    "\n",
    "    for wrong, right in misspellings.items():\n",
    "        text = text.replace(wrong, right)\n",
    "\n",
    "    # Remove \"customer\" or \"agent\" if they are at the end of the line\n",
    "    text = re.sub(r'\\b(customer|agent)\\b\\s*$', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33599a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "df['conversation'] = df['conversation'].astype(str).apply(clean_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3ecba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    df[['conversation', 'label']], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8a46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Train set distribution:\n",
      "label\n",
      "1    434\n",
      "0    329\n",
      "2     13\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.559\n",
      "0    0.424\n",
      "2    0.017\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ðŸ“Š Validation set distribution:\n",
      "label\n",
      "1    108\n",
      "0     82\n",
      "2      4\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.557\n",
      "0    0.423\n",
      "2    0.021\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save output\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "train_df.to_csv(\"data/train_cleaned.csv\", index=False)\n",
    "val_df.to_csv(\"data/val_cleaned.csv\", index=False)\n",
    "\n",
    "# Check class distributions\n",
    "print(\"ðŸ“Š Train set distribution:\")\n",
    "print(train_df['label'].value_counts())  # actual count\n",
    "print(train_df['label'].value_counts(normalize=True).round(3))  # proportions\n",
    "\n",
    "print(\"\\nðŸ“Š Validation set distribution:\")\n",
    "print(val_df['label'].value_counts())\n",
    "print(val_df['label'].value_counts(normalize=True).round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
